<p>I am a postdoctoral researcher at the <a href="https://mitibmwatsonailab.mit.edu/">MIT-IBM Watson AI Lab</a>.
  I received my PhD from the University of Edinburgh, under <a href="https://homepages.inf.ed.ac.uk/csutton/">Prof. Charles Sutton</a>, and my MSc in Computer Science from the University of Oxford. I have also completed research internships at Amazon and Meta.</p>

  <h2>Research Highlights</h2>
  <strong>Modular Continual Learning</strong> - showed that neurosymbolic methods can attain different types of knowledge transfer  
  (<a href="https://proceedings.neurips.cc/paper_files/paper/2018/hash/edc27f139c3b4e4bb29d1cdbc45663f9-Abstract.html"><span class="fa fa-graduation-cap fa-xs item-icon"></span></a>
  <a href="https://www.youtube.com/watch?v=lrZh4xgd_1s"><span class="fa fa-youtube fa-xs item-icon"></span></a>
  <a href="https://x.com/swarat/status/1038445537282936833"><span class="fa fa-twitter fa-xs item-icon"></span></a>)
  and can scale to large data streams
  (<a href="https://arxiv.org/abs/2306.06545"><span class="fa fa-graduation-cap fa-xs item-icon"></span></a>
  <a href="https://x.com/lazarvalkov/status/1787131326493032604"><span class="fa fa-twitter fa-xs item-icon"></span></a>
  <a href="https://iclr.cc/virtual/2024/poster/18820"><span class="fa  fa-file-powerpoint-o fa-xs item-icon"></span></a>).
  <br><br>
  Continual Pretraining - showed that continual pretraining of foundational models can be framed as a multi-armed bandit problem. The resulting method achieves SOTA results on reducing forgetting
  (<a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=GMeeGyEAAAAJ&amp;citation_for_view=GMeeGyEAAAAJ:LkGwnXOMwfcC"><span class="fa fa-graduation-cap fa-xs item-icon"></span></a>).
  <h2>Current Research Directions</h2>
    <ul>
      <li>Continual Learning</li>
      <li>Reasoning for LLMs</li>
    </ul>

<hr />
email: &ltfirst name&gtvalkov@gmail.com