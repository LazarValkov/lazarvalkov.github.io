<p>I am a postdoctoral researcher at the <a href="https://mitibmwatsonailab.mit.edu/">MIT-IBM Watson AI Lab</a>.
  I received my PhD from the University of Edinburgh, under <a href="https://homepages.inf.ed.ac.uk/csutton/">Prof. Charles Sutton</a>, and my MSc in Computer Science from the University of Oxford. I have also completed research internships at Amazon and Meta.</p>

  <h2>Research Highlights</h2>
  <strong>Modular Continual Learning</strong> - showed that neurosymbolic methods can attain different types of knowledge transfer  
  (<a href="https://proceedings.neurips.cc/paper_files/paper/2018/hash/edc27f139c3b4e4bb29d1cdbc45663f9-Abstract.html"><span class="fa fa-graduation-cap fa-xs item-icon"></span></a>
  <a href="https://www.youtube.com/watch?v=lrZh4xgd_1s"><span class="fa fa-youtube fa-xs item-icon"></span></a>
  <a href="https://x.com/swarat/status/1038445537282936833"><span class="fa fa-twitter fa-xs item-icon"></span></a>)
  and can scale to large data streams
  (<a href="https://arxiv.org/abs/2306.06545"><span class="fa fa-graduation-cap fa-xs item-icon"></span></a>
  <a href="https://x.com/lazarvalkov/status/1787131326493032604"><span class="fa fa-twitter fa-xs item-icon"></span></a>
  <a href="https://iclr.cc/virtual/2024/poster/18820"><span class="fa  fa-file-powerpoint-o fa-xs item-icon"></span></a>).
  <br><br>
  Continual Pretraining - showed that continual pretraining of foundational models can be framed as a multi-armed bandit problem. The resulting method achieves SOTA results on reducing forgetting
  (<a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=GMeeGyEAAAAJ&amp;citation_for_view=GMeeGyEAAAAJ:LkGwnXOMwfcC"><span class="fa fa-graduation-cap fa-xs item-icon"></span></a>).
  <h2>Research</h2>
    <p>
      <h3 style="display: inline;">Focus:</h3> Enhancing learning efficiency of neural networks to reduce their reliance on extensive data sets.<br/>
      <h3 style="display: inline;">Approach:</h3> Introducing inductive biases into models: <br/> 
      <ul>
        <li>by biasing a model's weights based on similar tasks</li>
        <li>by augmenting a model's architecture using neurosymbolic methods.</li>
      <!--</ul>1) by biasing a model's weights based on past similar tasks; <br/> 2) by incorporating expert knowledge through neurosymbolic methods to augment the model's architecture. <br/>!-->
      </ul>
      <h3 style="display: inline;">Philosophy:</h3> I aim to develop principled methods by initially tackling simpler problem instances, then scaling these approaches to more complex scenarios.
      <!--My research in AI focuses on enhancing learning efficiency by introducing inductive biases into models.
      This includes biasing a model's weights for specific tasks based on past similar tasks, or incorporating expert knowledge through neurosymbolic methods to improve model architecture.
      By enhancing learning efficiency, we reduce the reliance on extensive data sets, thus broadening the applicability of AI solutions.
      My approach involves first understanding simpler problem instances and then developing principled methods that can be scaled to more complex scenarios.
      
      My research in AI focuses on enhancing the learning efficiency by introducing inductive bias into models. This includes biasing the model weights for specific tasks based on similar tasks, or incorporating expert knowledge using neurosymbolic methods to augment the model architecture.
      Enhancing the learning efficiency alleviates the need for extensive data, thus, broadening the applicability of AI solutions.
      My preferred approach is to understand simpler prolbem instances and devise a principled method which is applicable to harder settings.
      I aim to develop principled methods by initially tackling simpler problem instances, then scaling these approaches to more complex scenarios.
      
      This not only reduces the need for extensive data but also broadens the applicability of AI solutions. My passion for [personal motivation] drives my commitment to advancing this field.
      My research has focused on reducing the sample complexity by introducing inductive bias into the model -- by biasing the weights based on similar tasks or by using expert knowledge and neurosymbolic methods to augment the model's architecture. 
      My preferred approach is to understand simpler instances of a problem and devise a principled approach which is applicable to harder settings.-->
    </p>
    Current Research Directions:
    <ul>
      <li>Continual Learning</li>
      <li>Reasoning for LLMs</li>
    </ul>
<hr />
email: &ltfirst name&gtvalkov@gmail.com